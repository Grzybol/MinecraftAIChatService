# Lokalny model LLM (llama.cpp / GGUF)
LLM_MODEL_PATH=/models/deepseek-1b.gguf
LLM_COMMAND=llama-cli
LLM_MAX_RAM_MB=1024
LLM_NUM_THREADS=6
LLM_CTX_SIZE=2048
LLM_TIMEOUT_MS=2000
LLM_TEMPERATURE=0.6
LLM_TOP_P=0.9
