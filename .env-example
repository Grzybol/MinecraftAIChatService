# Lokalny model LLM (llama.cpp / GGUF)
LLM_MODEL_PATH=/models/deepseek-1b.gguf
LLM_MODELS_DIR=/models
LLM_SERVER_URL=http://127.0.0.1:8080
LLM_SERVER_COMMAND=llama-server
LLM_COMMAND=llama-cli
LLM_MAX_RAM_MB=1024
LLM_NUM_THREADS=6
LLM_CTX_SIZE=2048
LLM_TIMEOUT_MS=2000
LLM_SOFT_TIMEOUT_MS=1000
LLM_SERVER_STARTUP_TIMEOUT_MS=60000
LLM_TEMPERATURE=0.6
LLM_TOP_P=0.9

# Logowanie
# LOG_LEVEL: poziom logów na stdout (domyślnie INFO)
# LOG_FILE_LEVEL: poziom logów w pliku (domyślnie jak LOG_LEVEL)
LOG_LEVEL=INFO
LOG_FILE_LEVEL=DEBUG
